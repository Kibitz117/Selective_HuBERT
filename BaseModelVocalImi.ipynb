{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b61683db-4a0c-4cf0-9e27-6376017eee1b",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-26 16:55:46.966132: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import os\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "from tqdm.autonotebook import tqdm, trange\n",
    "from transformers.optimization import AdamW\n",
    "from transformers import HubertModel, AutoConfig\n",
    "\n",
    "from models.hubert_selective import HuBERTSelectiveNet\n",
    "from utils.model_tools import *\n",
    "from utils.selective_loss import SelectiveLoss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "84f77867-9b85-4c40-8549-ce79b3fc9a69",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n"
     ]
    }
   ],
   "source": [
    "labels = np.load(\"data/queen_and_no_queen_labels.npy\")\n",
    "print(labels[:20])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "67bea4e8-4e5a-4b43-ad91-5f63fb8a9bc3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef002292-9f34-4b5c-9fb7-07e1257fb703",
   "metadata": {},
   "source": [
    "### Loading in Vocal Imitation Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b06c24f4-cc30-408d-b2b8-c1554bfd73ea",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data/vocal_imitation-full/labelvocabulary.csv\n",
      "data/vocal_imitation-full/fold00.json\n",
      "data/vocal_imitation-full/labelvocabulary.csv\n",
      "data/vocal_imitation-full/fold01.json\n",
      "data/vocal_imitation-full/labelvocabulary.csv\n",
      "data/vocal_imitation-full/fold02.json\n",
      "302\n"
     ]
    }
   ],
   "source": [
    "data_dir = 'data/vocal_imitation-full'\n",
    "model_id = \"facebook/hubert-base-ls960\"\n",
    "\n",
    "from transformers import Wav2Vec2FeatureExtractor\n",
    "\n",
    "def prepare_dataset(batch, feature_extractor):\n",
    "    waveforms, labels = zip(*batch)\n",
    "    waveforms = list(waveforms)\n",
    "    features = feature_extractor(waveforms, return_tensors=\"pt\", padding=True, sampling_rate=16000)\n",
    "    labels = torch.tensor(labels)\n",
    "    \n",
    "    return features, labels\n",
    "\n",
    "hubert_model = HubertModel.from_pretrained(model_id)\n",
    "feature_extractor = Wav2Vec2FeatureExtractor.from_pretrained(model_id)\n",
    "\n",
    "fold00 = VocalImitationDataset(data_dir, fold_name='fold00')\n",
    "fold01 = VocalImitationDataset(data_dir, fold_name='fold01')\n",
    "fold02 = VocalImitationDataset(data_dir, fold_name='fold02')\n",
    "\n",
    "num_classes = len(fold00.vocab_list)\n",
    "print(num_classes)\n",
    "fold_set = set([fold00, fold01, fold02])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "975f189a-e563-403a-85a4-1f390a1eab18",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import HubertForSequenceClassification, HubertConfig\n",
    "\n",
    "class HuBERTSelectiveNet(torch.nn.Module):\n",
    "    def __init__(self, hubert_model, num_classes:int,feature_size:int, init_weights=True):\n",
    "        super(HuBERTSelectiveNet, self).__init__()\n",
    "        self.hubert_model = hubert_model\n",
    "        self.dim_features = feature_size  # This should be 768 based on your config\n",
    "        self.num_classes = num_classes\n",
    "        \n",
    "        # Classifier represented as f() in the original paper\n",
    "        self.classifier = torch.nn.Sequential(\n",
    "            torch.nn.Linear(self.dim_features, self.num_classes)\n",
    "        )\n",
    "\n",
    "        self.selector = torch.nn.Sequential(\n",
    "            torch.nn.Linear(self.dim_features, self.dim_features),\n",
    "            torch.nn.ReLU(True),\n",
    "            # Normalize across the feature dimension, which is the last dimension of the input\n",
    "            torch.nn.BatchNorm1d(self.dim_features), # self.dim_features should be 768\n",
    "            torch.nn.Linear(self.dim_features, 1),\n",
    "            torch.nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "        # Auxiliary classifier represented as h() in the original paper\n",
    "        self.aux_classifier = torch.nn.Sequential(\n",
    "            torch.nn.Linear(self.dim_features, self.num_classes)\n",
    "        )\n",
    "\n",
    "        #Initialize weights of heads if required\n",
    "        if init_weights:\n",
    "            self._initialize_weights(self.classifier)\n",
    "            self._initialize_weights(self.selector)\n",
    "            self._initialize_weights(self.aux_classifier)\n",
    "\n",
    "    def forward(self, input_values):\n",
    "        # Run input through HuBERT model\n",
    "        outputs = self.hubert_model(input_values)\n",
    "\n",
    "        # Extract the last hidden state (features)\n",
    "        x = outputs.last_hidden_state  # Extracts the tensor\n",
    "\n",
    "        # Perform mean pooling over the timesteps\n",
    "        # Assuming x has shape [batch_size, num_timesteps, num_features]\n",
    "        x = torch.mean(x, dim=1)  # Now x has shape [batch_size, num_features]\n",
    "\n",
    "        # Pass the pooled features through the classifier and selector heads\n",
    "        prediction_out = self.classifier(x)\n",
    "        selection_out = self.selector(x)\n",
    "        auxiliary_out = self.aux_classifier(x)\n",
    "\n",
    "        return prediction_out, selection_out, auxiliary_out\n",
    "\n",
    "    def _initialize_weights(self, module):\n",
    "        for m in module.modules():\n",
    "            if isinstance(m, torch.nn.Conv2d):\n",
    "                torch.nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')\n",
    "                if m.bias is not None:\n",
    "                    torch.nn.init.constant_(m.bias, 0)\n",
    "            elif isinstance(m, torch.nn.BatchNorm1d):\n",
    "                torch.nn.init.constant_(m.weight, 1)\n",
    "                torch.nn.init.constant_(m.bias, 0)\n",
    "            elif isinstance(m, torch.nn.Linear):\n",
    "                torch.nn.init.normal_(m.weight, 0, 0.01)\n",
    "                torch.nn.init.constant_(m.bias, 0)\n",
    "    def save_model(model, save_path, model_config):\n",
    "        # Save the model state\n",
    "        torch.save(model.state_dict(), save_path + \"/model_state.pt\")\n",
    "\n",
    "        # Save the configuration\n",
    "        with open(save_path + \"/config.json\", 'w') as f:\n",
    "            json.dump(model_config, f)\n",
    "    def load_model(load_path, hubert_model_class):\n",
    "        # Load the configuration\n",
    "        with open(load_path + \"/config.json\", 'r') as f:\n",
    "            model_config = json.load(f)\n",
    "\n",
    "        # Recreate the HuBERTSelectiveNet instance\n",
    "        hubert_model = HubertModel.from_pretrained(\"facebook/hubert-base-ls960\")\n",
    "        model = hubert_model_class(hubert_model, model_config[\"num_classes\"], model_config[\"feature_size\"])\n",
    "\n",
    "        # Load the model state\n",
    "        model.load_state_dict(torch.load(load_path + \"/model_state.pt\"))\n",
    "        model.eval()\n",
    "        return model\n",
    "\n",
    "def selective_train(dataloader, model, selective_loss, optimizer, device) -> float:\n",
    "    size = len(dataloader.dataset)\n",
    "    train_loss = 0.0\n",
    "\n",
    "    model.train()\n",
    "    for batch, (X, y) in enumerate(dataloader):\n",
    "        X, y = X.to(device), y.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        logits, selection_logits, auxiliary_logits = model(X)\n",
    "        #auxiliary_logits = auxiliary_logits.mean(dim=1) # why were we doing this?\n",
    "        labels = y.long()\n",
    "        loss_dict = selective_loss(prediction_out=logits,\n",
    "                                    selection_out=selection_logits,\n",
    "                                    auxiliary_out=auxiliary_logits,\n",
    "                                    target=labels)\n",
    "\n",
    "        loss = loss_dict['loss']\n",
    "        loss.backward()\n",
    "        \n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "        \n",
    "        optimizer.step()\n",
    "\n",
    "        # Append lists\n",
    "        train_loss += loss.item()\n",
    "\n",
    "        if batch % 1000 == 0:\n",
    "            loss, current = loss.item(), batch * len(X)\n",
    "            print(f\"loss: {loss:>7f}  [{current:>5d}/{size:>5d}]\")\n",
    "\n",
    "    return train_loss/len(dataloader)\n",
    "\n",
    "class SelectiveLoss(torch.nn.Module):\n",
    "    def __init__(self, loss_func, coverage:float, alpha:float=0.5, lm:float=32.0, device='cpu'):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            loss_func: base loss function. the shape of loss_func(x, target) shoud be (B). \n",
    "                       e.g.) torch.nn.CrossEntropyLoss(reduction=none) : classification\n",
    "            coverage: target coverage.\n",
    "            lm: Lagrange multiplier for coverage constraint. original experiment's value is 32. \n",
    "        \"\"\"\n",
    "        super(SelectiveLoss, self).__init__()\n",
    "        assert 0.0 < coverage <= 1.0\n",
    "        assert 0.0 < lm\n",
    "        assert 0.0 < alpha <= 1.0\n",
    "\n",
    "        self.loss_func = loss_func\n",
    "        self.coverage = coverage\n",
    "        self.lm = lm\n",
    "        self.alpha = alpha\n",
    "\n",
    "    def forward(self, prediction_out, selection_out, auxiliary_out, target, threshold=0.5, mode='train'):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            prediction_out: (B, num_classes)\n",
    "            selection_out:  (B, 1)\n",
    "            auxiliary_out:\n",
    "            target:\n",
    "            threshold:\n",
    "            mode: str (train/test)\n",
    "        \"\"\"\n",
    "        \n",
    "        # selection is of size batch, features but prediction and aux should be of size batch, classes\n",
    "        \n",
    "        cross_entropy = torch.nn.CrossEntropyLoss()\n",
    "        \n",
    "        # compute empirical coverage (=phi^)\n",
    "        empirical_coverage = selection_out.mean() \n",
    "        \n",
    "        # compute empirical risk (=r^)\n",
    "        empirical_risk = (self.loss_func(prediction_out, target) * selection_out.view(-1)).mean()\n",
    "        empirical_risk = empirical_risk / empirical_coverage\n",
    "\n",
    "        # compute penalty (=psi)\n",
    "        coverage = torch.tensor([self.coverage], dtype=torch.float32, requires_grad=True, device=device) # can I just put this on the stupid device beforehand\n",
    "        penalty = torch.max(coverage - empirical_coverage, \n",
    "                            torch.tensor([0.0], dtype=torch.float32, requires_grad=True, device=device)) ** 2\n",
    "        penalty *= self.lm\n",
    "\n",
    "        # compute selective loss (=L(f,g))\n",
    "        selective_loss = empirical_risk + penalty\n",
    "        \n",
    "        # Assuming binary classification\n",
    "        auxiliary_out_expanded = auxiliary_out#torch.stack([auxiliary_out, -auxiliary_out], dim=1)\n",
    "\n",
    "        # Now compute the cross entropy loss\n",
    "        ce_loss = cross_entropy(auxiliary_out_expanded, target)\n",
    "\n",
    "        \n",
    "        # total loss\n",
    "        loss_pytorch = self.alpha * selective_loss + (1.0 - self.alpha) * ce_loss\n",
    "        \n",
    "        # compute coverage based on source implementation\n",
    "        selective_head_coverage = self.get_coverage(selection_out, threshold)\n",
    "\n",
    "        # compute selective accuracy based on source implementation\n",
    "        selective_head_selective_acc = self.get_selective_acc(prediction_out, selection_out, target)\n",
    "\n",
    "        # compute accuracy based on source implementation\n",
    "        classification_head_acc = self.get_accuracy(auxiliary_out, target)\n",
    "        \n",
    "        # compute selective loss (=selective_head_loss) based on source implementation\n",
    "        selective_head_loss = self.get_selective_loss(prediction_out, selection_out, target)\n",
    "\n",
    "        # compute cross entropy loss (=classification_head_loss) based on source implementation\n",
    "        classification_head_loss = cross_entropy(auxiliary_out_expanded, target)\n",
    "\n",
    "        # compute loss\n",
    "        loss = self.alpha * selective_head_loss + (1.0 - self.alpha) * classification_head_loss\n",
    "\n",
    "        # empirical selective risk with rejection for test model\n",
    "        if mode == 'test':\n",
    "            test_selective_risk = self.get_selective_risk(prediction_out, selection_out, target, threshold) \n",
    "\n",
    "        # loss information dict \n",
    "        pref = ''\n",
    "        if mode == 'validation':\n",
    "            pref = 'val_'\n",
    "        loss_dict={}\n",
    "        loss_dict['{}empirical_coverage'.format(pref)] = empirical_coverage.detach().cpu().item()\n",
    "        loss_dict['{}empirical_risk'.format(pref)] = empirical_risk.detach().cpu().item()\n",
    "        loss_dict['{}penalty'.format(pref)] = penalty.detach().cpu().item()\n",
    "        loss_dict['{}selective_loss'.format(pref)] = selective_loss.detach().cpu().item()\n",
    "        loss_dict['{}ce_loss'.format(pref)] = ce_loss.detach().cpu().item()\n",
    "        loss_dict['{}loss_pytorch'.format(pref)] = loss_pytorch\n",
    "        loss_dict['{}selective_head_coverage'.format(pref)] = selective_head_coverage.detach().cpu().item() #coverage\n",
    "        loss_dict['{}selective_head_selective_acc'.format(pref)] = selective_head_selective_acc.detach().cpu().item() #selective_accurcy\n",
    "        loss_dict['{}classification_head_acc'.format(pref)] = classification_head_acc.detach().cpu().item() #calassification_accuracy\n",
    "        loss_dict['{}selective_head_loss'.format(pref)] = selective_head_loss.detach().cpu().item() #selective_loss\n",
    "        loss_dict['{}classification_head_loss'.format(pref)] = classification_head_loss.detach().cpu().item() #ce_loss\n",
    "        loss_dict['{}loss'.format(pref)] = loss\n",
    "        if mode == 'test':\n",
    "            loss_dict['test_selective_risk'] = test_selective_risk.detach().cpu().item()\n",
    "\n",
    "        return loss_dict\n",
    "\n",
    "    # based on source implementation\n",
    "    def get_selective_acc(self, prediction_out, selection_out, target):\n",
    "        \"\"\"\n",
    "        Equivalent to selective_acc function of source implementation\n",
    "        Args:\n",
    "            prediction_out: (B,num_classes)\n",
    "            selection_out:  (B, 1)\n",
    "        \"\"\"\n",
    "        g = (selection_out.mean(dim=1).squeeze(-1) > 0.5).float()\n",
    "        num = torch.dot(g, (torch.argmax(prediction_out, dim=-1) == target).float())\n",
    "        return num / torch.sum(g)\n",
    "\n",
    "    # based on source implementation\n",
    "    def get_coverage(self, selection_out, threshold):\n",
    "        \"\"\"\n",
    "        Equivalent to coverage function of source implementation\n",
    "        Args:\n",
    "            selection_out:  (B, 1)\n",
    "        \"\"\"\n",
    "        g = (selection_out.squeeze(-1) >= threshold).float()\n",
    "        return torch.mean(g)\n",
    "\n",
    "    # based on source implementation\n",
    "    def get_accuracy(self, auxiliary_out, target): #TODO: Check implementation with Lili\n",
    "        \"\"\"\n",
    "        Equivalent to \"accuracy\" in Tensorflow\n",
    "        Args:\n",
    "            selection_out:  (B, 1)\n",
    "        \"\"\" \n",
    "        num = torch.sum((torch.argmax(auxiliary_out, dim=-1) == target).float())\n",
    "        return num / len(auxiliary_out)\n",
    "    \n",
    "    # based on source implementation\n",
    "    def get_selective_loss(self, prediction_out, selection_out, target):\n",
    "        \"\"\"\n",
    "        Equivalent to selective_loss function of source implementation\n",
    "        Args:\n",
    "            prediction_out: (B,num_classes)\n",
    "            selection_out:  (B, 1)\n",
    "        \"\"\"\n",
    "        device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "        ce = self.loss_func(prediction_out, target)\n",
    "        empirical_risk_variant = torch.mean(ce * selection_out.view(-1))\n",
    "        empirical_coverage = selection_out.mean() \n",
    "        penalty = torch.max(self.coverage - empirical_coverage, torch.tensor([0.0], dtype=torch.float32, requires_grad=True, device=device))**2\n",
    "        loss = empirical_risk_variant + self.lm * penalty\n",
    "        return loss\n",
    "\n",
    "    # selective risk in test mode\n",
    "    def get_selective_risk(self, prediction_out, selection_out, target, threshold):\n",
    "        g = (selection_out.squeeze(-1) >= threshold).float()\n",
    "        empirical_coverage_rjc = torch.mean(g)\n",
    "        empirical_risk_rjc = torch.mean(self.loss_func(prediction_out, target) * g.view(-1))\n",
    "        empirical_risk_rjc /= empirical_coverage_rjc\n",
    "        return empirical_risk_rjc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "5ff4f4f2-4ab8-46e4-9641-5786e54a7afb",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "features: 768\n",
      "302\n"
     ]
    }
   ],
   "source": [
    "# Instantiate hubert model and make the full selectivenet\n",
    "\n",
    "# I wonder if I will have a problem with this featuresize if I need to do padding.\n",
    "\n",
    "inputs, labels = next(iter(fold00))\n",
    "inputs = feature_extractor(inputs, return_tensors=\"pt\", sampling_rate=16000)\n",
    "outputs = hubert_model(inputs.input_values)\n",
    "feature_size = outputs.last_hidden_state.shape[2]\n",
    "print('features:', feature_size)\n",
    "\n",
    "model = HuBERTSelectiveNet(hubert_model, num_classes=num_classes, feature_size=feature_size)\n",
    "print(model.num_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "b6439edf-39b9-4e8c-9116-db57d238c365",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_func = torch.nn.CrossEntropyLoss()\n",
    "coverage = 0.8\n",
    "alpha = 0.5\n",
    "lm = 32.0\n",
    "num_epochs = 10\n",
    "batch_size = 10\n",
    "\n",
    "loss_fn = SelectiveLoss(loss_func, coverage, alpha, lm)\n",
    "    \n",
    "# loss=loss\n",
    "# Create the optimizer\n",
    "optimizer = AdamW(model.parameters(), lr=1e-4)\n",
    "\n",
    "model_file = 'models/selective-hubert-10ep-80c.pt' # 80c is 80% coverage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2464625c-7e87-4796-b7c5-e014ac286e9d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "173da2c70f48436e97de13cd37f606cd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1\n",
      "-------------------------------\n"
     ]
    }
   ],
   "source": [
    "train_losses_file = 'logs/selective-hubert-10ep-80c-train.txt'\n",
    "test_losses_file = 'logs/selective-hubert-10ep-80c-test.txt'\n",
    "\n",
    "train_losses = []\n",
    "test_losses = []\n",
    "t = trange(num_epochs)\n",
    "\n",
    "for fold in fold_set:\n",
    "    off_folds = fold_set.difference([fold])\n",
    "    off_concat = torch.utils.data.ConcatDataset(off_folds)\n",
    "    \n",
    "    train_loader = DataLoader(fold, batch_size=batch_size, shuffle=True)\n",
    "    test_loader = DataLoader(off_concat, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "    for epoch in t:\n",
    "        print(f\"Epoch {epoch+1}\\n-------------------------------\")\n",
    "        train_loss = selective_train(train_loader, model, loss_fn, optimizer, device)\n",
    "        test_loss = selective_test(test_loader, model, device)\n",
    "        train_losses.append(train_loss)\n",
    "        test_losses.append(test_loss)\n",
    "        \n",
    "        torch.save(model.state_dict(), model_file)\n",
    "        \n",
    "with open(train_losses_file, 'w') as fp:\n",
    "    for s in train_losses:\n",
    "        fp.write(\"%s\\n\" % s)\n",
    "        \n",
    "with open(test_losses_file, 'w') as fp:\n",
    "    for x in test_losses:\n",
    "        fp.write(\"%s\\n\" % x)\n",
    "\n",
    "print(\"Done!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5b44ab4-2636-46c1-a2a7-cf87741b1eb3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
